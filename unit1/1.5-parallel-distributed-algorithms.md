# Parallel and Distributed Algorithms

Task: Investigate the impact of parallel and distributed algorithms on modern computing. Provide a Rust implementation of a simple parallel algorithm, and discuss the benefits and challenges of parallelism in algorithm design.

Parallel and distributed algorithms underlie almost every high-performance system today—from multi-core CPUs in your laptop to massive clusters powering web search, machine learning training, and big-data analytics. By splitting work across multiple processing units (threads, cores, machines), we can tackle larger datasets, shave down runtimes, and improve responsiveness. But parallelism also brings new headaches around synchronization, communication overhead, and non-determinism.

---

## 1. Impact on Modern Computing

* **Scaling with hardware**
  Moore’s Law slowed, but core counts kept climbing. Parallel algorithms let us keep wringing performance gains out of hardware by dividing work across those cores.
* **Big data & distributed systems**
  Frameworks like MapReduce and Spark turn clusters of commodity servers into a single “computer,” letting us process petabytes of logs, train huge ML models, or run graph analytics at web scale.
* **Real-time responsiveness**
  On the client side, parallelism powers smooth UI animations, real-time physics in games, and low-latency data processing in databases and search engines.

---

## 2. Simple Parallel Sum in Rust

Below is a toy example that sums a large slice of `f64` values by splitting the slice in half and summing each half on its own thread. In real code you’d reach for a library like Rayon, but this shows the core idea:

```rust
use std::thread;

/// Sum slice in parallel by dividing into two halves.
fn parallel_sum(data: &[f64]) -> f64 {
    let mid = data.len() / 2;
    let (left, right) = data.split_at(mid);

    // Spawn a thread to sum the left half
    let left_handle = thread::spawn(move || left.iter().copied().sum::<f64>());

    // Meanwhile sum the right half in this thread
    let right_sum: f64 = right.iter().copied().sum();

    // Wait for the left thread and add up
    let left_sum = left_handle.join().unwrap();
    left_sum + right_sum
}

fn main() {
    let huge_vec: Vec<f64> = (0..10_000_000).map(|i| i as f64).collect();
    let total = parallel_sum(&huge_vec);
    println!("Sum = {}", total);
}
```

* **How it works**:

  1. **Divide** the data into two halves.
  2. **Conquer** by spawning a thread to process one half while the main thread handles the other.
  3. **Combine** by joining the thread and summing the two partial results.

---

## 3. Benefits of Parallelism

1. **Throughput & latency improvements**

   * Many tasks become noticeably faster—wall-clock time drops roughly in proportion to the number of cores (up to a point).
2. **Resource utilization**

   * Keeps multiple CPU cores busy instead of letting them idle.
3. **Scalability**

   * Distributed algorithms can scale out across machines, handling huge datasets that won’t fit on a single box.

---

## 4. Challenges & Pitfalls

1. **Amdahl’s Law**

   * Speed-up is limited by the portion of the code that *can’t* be parallelized. If 10% of your work is strictly serial, the maximum theoretical speed-up is 10×, no matter how many cores you throw at it.
2. **Overhead**

   * Spawning threads, synchronizing results, and passing messages all consume time and memory. For small tasks, the overhead can outweigh the benefit.
3. **Data races & deadlocks**

   * Unsynchronized access to shared memory can corrupt data. Poorly designed locking or communication patterns can lead to deadlock or livelock.
4. **Load balancing**

   * Uneven work splits can leave some threads idle while others slog away. In distributed systems, node failures or stragglers can bottleneck the whole job.
5. **Non-determinism & debugging pain**

   * Bugs can be intermittent and hard to reproduce; race conditions may only surface under specific timing.

---

## 5. When to Reach for Parallel/Distributed Solutions

* **CPU-bound workloads** on multi-core machines (e.g., image processing, numerical simulations).
* **Embarrassingly parallel tasks** where subproblems don’t interact (e.g., Monte Carlo simulations, batch data transforms).
* **Large-scale data processing** that needs to exceed a single machine’s memory or CPU.
* **Low-latency streaming** where pipelining and parallel operators keep throughput high.

But for small or highly sequential tasks, simple single-threaded code often wins on clarity, reliability, and reduced overhead. The sweet spot is where you can expose enough independent work to amortize parallelism’s costs and where algorithmic structure admits clean splits.
