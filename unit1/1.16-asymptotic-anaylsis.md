# Asymtotic Analysis in Development and Comparison of Algorithms

Task: Analyze the role of asymptotic analysis (Big O notation) in the development and comparison of algorithms. Provide examples of different algorithms in Rust, and use Big O notation to explain their efficiency and potential bottlenecks.

Asymptotic analysis—using Big O notation—is your go-to tool for answering “How will this algorithm behave as my data grows?” It lets you:

* **Compare at scale**, independent of hardware or language specifics.
* **Spot bottlenecks** early (“This’ll blow up if n doubles!”).
* **Guide design choices**, e.g. “Is the extra complexity of a fancy algorithm worth it for my data sizes?”

Below are a handful of Rust snippets illustrating common patterns, their Big O costs, and where you’d hit the wall.

---

## 1. Linear Search (`O(n)` time, `O(1)` space)

Scan every element until you find your target.

```rust
fn linear_search<T: PartialEq>(arr: &[T], target: &T) -> Option<usize> {
    for (i, item) in arr.iter().enumerate() {
        if item == target {
            return Some(i);
        }
    }
    None
}

fn example_linear() {
    let data = (0..1_000_000).collect::<Vec<_>>();
    // Worst case: examines all 1 000 000 entries → linear time
    println!("{:?}", linear_search(&data, &999_999));
}
```

* **Time:** O(n) — double n, double the work.
* **Space:** O(1) extra.
* **Bottleneck:** For really large n (millions+), a single pass can be noticeable.

---

## 2. Binary Search (`O(log n)` time, `O(1)` space)

Jump to the middle, cut the search space in half each step—requires a **sorted** slice.

```rust
fn binary_search(arr: &[i32], target: i32) -> Option<usize> {
    let mut lo = 0;
    let mut hi = arr.len();
    while lo < hi {
        let mid = lo + (hi - lo) / 2;
        match arr[mid].cmp(&target) {
            std::cmp::Ordering::Less => lo = mid + 1,
            std::cmp::Ordering::Greater => hi = mid,
            std::cmp::Ordering::Equal => return Some(mid),
        }
    }
    None
}

fn example_binary() {
    let data = (0..1_000_000).collect::<Vec<_>>(); // already sorted
    // 20 comparisons max, since log2(1e6) ≈ 20
    println!("{:?}", binary_search(&data, 123456));
}
```

* **Time:** O(log n) — double n, add just one more comparison.
* **Space:** O(1).
* **Bottleneck:** You pay up-front cost to sort (O(n log n)) if data isn’t already ordered.

---

## 3. Merge Sort (`O(n log n)` time, `O(n)` space)

A classic divide-and-conquer sort.

```rust
fn merge_sort<T: Ord + Clone>(arr: &[T]) -> Vec<T> {
    let n = arr.len();
    if n <= 1 { return arr.to_vec() }
    let mid = n / 2;
    let left = merge_sort(&arr[..mid]);
    let right = merge_sort(&arr[mid..]);

    let mut merged = Vec::with_capacity(n);
    let (mut i, mut j) = (0, 0);
    while i < left.len() && j < right.len() {
        if left[i] <= right[j] {
            merged.push(left[i].clone());
            i += 1;
        } else {
            merged.push(right[j].clone());
            j += 1;
        }
    }
    merged.extend_from_slice(&left[i..]);
    merged.extend_from_slice(&right[j..]);
    merged
}

fn example_merge() {
    let mut data = (0..100_000).rev().collect::<Vec<_>>();
    let sorted = merge_sort(&data);
    println!("First 5: {:?}", &sorted[..5]);
}
```

* **Time:** O(n log n) always.
* **Space:** O(n) extra buffer.
* **Bottleneck:** Memory traffic and allocations—on small n, the constant factors can beat a simple O(n²) sort like insertion sort.

---

## 4. HashMap Lookup (`O(1)` avg, `O(n)` worst)

Constant-time lookups on average, but pathological keys or rehashing can push it toward linear.

```rust
use std::collections::HashMap;

fn example_hashmap() {
    let mut map = HashMap::with_capacity(1_000_000);
    for i in 0..1_000_000 {
        map.insert(i, format!("Value {}", i));
    }
    // Avg O(1); worst-case O(n) if every key collides into one bucket
    if let Some(v) = map.get(&123_456) {
        println!("{}", v);
    }
}
```

* **Time:** O(1) average lookup/insert/remove.
* **Space:** O(n).
* **Bottleneck:** Watch out for rehash thresholds (costly spikes) and attacks that force collisions.

---

## 5. Naïve Fibonacci (`O(2ⁿ)` time, `O(n)` space)

A great cautionary tale: recursion with overlapping subproblems explodes exponentially.

```rust
fn fib(n: u32) -> u64 {
    match n {
        0 => 0,
        1 => 1,
        _ => fib(n-1) + fib(n-2),
    }
}

fn example_fib() {
    // fib(40) already takes seconds!
    println!("{}", fib(30));
}
```

* **Time:** O(2ⁿ) — every increase doubles the calls.
* **Space:** O(n) for the call stack.
* **Bottleneck:** Completely impractical past n≈40–45; switch to dynamic programming (O(n) time, O(n) space) or matrix exponentiation (O(log n)).

---

## Putting It All Together

1. **Big O isn’t everything**, but it tells you the *shape* of scaling: linear vs. logarithmic vs. exponential.
2. **Choose wisely**:

   * If you only ever search tiny arrays, O(n) vs. O(log n) might not matter.
   * If you sort millions of items daily, shaving off log n or constant factors pays dividends.
3. **Watch the gotchas**:

   * Worst‐case vs. average‐case (hash tables).
   * Space costs—extra buffers, recursion depth.
   * Hidden preconditions—sorted data for binary search, memory availability for merge sort.

By lining up your algorithm’s Big O profiles against your data sizes and hardware envelope, you’ll catch scalability problems on paper long before they crash your production servers.
