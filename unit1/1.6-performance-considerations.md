# Performance Considerations on CPU, GPU, and Embedded Systems

Task: Evaluate the performance considerations when designing algorithms for specific hardware environments such as CPUs, GPUs, and embedded systems. Implement an algorithm in Rust and optimize it for different hardware, explaining the trade-offs involved.

1. **Baseline (single-threaded CPU)**
2. **Multi-core CPU (parallel + SIMD)**
3. **GPU (OpenCL)**
4. **Embedded (no\_std, fixed-point)**

…then discuss the trade-offs for each.

---

## 1. Baseline: Single-Threaded CPU

```rust
/// Compute dot-product of `a` and `b` in O(n) time, O(1) extra space.
fn dot_product(a: &[f32], b: &[f32]) -> f32 {
    a.iter()
     .zip(b.iter())
     .map(|(x, y)| x * y)
     .sum()
}

fn main() {
    let n = 1_000_000;
    let a: Vec<_> = (0..n).map(|i| i as f32).collect();
    let b: Vec<_> = (0..n).map(|i| (n - i) as f32).collect();
    let result = dot_product(&a, &b);
    println!("dot = {}", result);
}
```

* **Pros:** trivial, minimal dependencies.
* **Cons:** only uses one core; doesn’t exploit SIMD registers.

---

## 2. Multi-Core CPU with SIMD and Rayon

```rust
use rayon::prelude::*;
use std::simd::{f32x8, SimdFloat};

fn dot_product_parallel(a: &[f32], b: &[f32]) -> f32 {
    // Chunk into blocks of 8 for SIMD
    let simd_sum: f32 = a
        .par_chunks(8)
        .zip(b.par_chunks(8))
        .map(|(ca, cb)| {
            let va = f32x8::from_slice_unaligned(ca);
            let vb = f32x8::from_slice_unaligned(cb);
            (va * vb).reduce_sum()
        })
        .sum();

    // Handle any leftovers
    let rem = a.len() % 8;
    let tail_start = a.len() - rem;
    let tail_sum: f32 = a[tail_start..]
        .iter()
        .zip(b[tail_start..].iter())
        .map(|(x, y)| x * y)
        .sum();

    simd_sum + tail_sum
}

fn main() {
    let n = 1_000_000;
    let a: Vec<_> = (0..n).map(|i| i as f32).collect();
    let b: Vec<_> = (0..n).map(|i| (n - i) as f32).collect();
    let result = dot_product_parallel(&a, &b);
    println!("parallel dot = {}", result);
}
```

* **Pros:**

  * **Thread-level** parallelism (Rayon) uses all cores.
  * **SIMD** f32x8 does eight multiplies/adds per instruction.
* **Cons:**

  * Overhead from spawning tasks—beneficial only on large inputs.
  * Must handle alignment or leftovers.

---

## 3. GPU with OpenCL

> *Note: you’ll need the `ocl` crate and an OpenCL-capable device.*

```rust
use ocl::{ProQue, Buffer};

const KERNEL_SRC: &str = r#"
__kernel void dot(__global const float* a,
                  __global const float* b,
                  __global float* partial,
                  const unsigned int n) {
    int gid = get_global_id(0);
    int stride = get_global_size(0);
    float sum = 0.0f;
    for (int i = gid; i < n; i += stride) {
        sum += a[i] * b[i];
    }
    partial[gid] = sum;
}
"#;

fn gpu_dot(a: &[f32], b: &[f32]) -> ocl::Result<f32> {
    let n = a.len() as u32;
    let pro_que = ProQue::builder()
        .src(KERNEL_SRC)
        .dims(n)
        .build()?;

    let buffer_a = Buffer::<f32>::builder()
        .queue(pro_que.queue().clone())
        .flags(ocl::flags::MEM_READ_ONLY)
        .len(n)
        .copy_host_slice(a)
        .build()?;
    let buffer_b = Buffer::<f32>::builder()
        .queue(pro_que.queue().clone())
        .flags(ocl::flags::MEM_READ_ONLY)
        .len(n)
        .copy_host_slice(b)
        .build()?;
    let buffer_part = Buffer::<f32>::builder()
        .queue(pro_que.queue().clone())
        .flags(ocl::flags::MEM_WRITE_ONLY)
        .len(pro_que.dims().to_len())
        .build()?;

    let kernel = pro_que.kernel_builder("dot")
        .arg(&buffer_a)
        .arg(&buffer_b)
        .arg(&buffer_part)
        .arg(&n)
        .build()?;
    unsafe { kernel.enq()?; }

    // Read back partial sums and combine on CPU
    let mut partials = vec![0.0f32; pro_que.dims().to_len()];
    buffer_part.read(&mut partials).enq()?;
    Ok(partials.iter().copied().sum())
}

fn main() -> ocl::Result<()> {
    let n = 10_000_000;
    let a: Vec<_> = (0..n).map(|i| i as f32).collect();
    let b: Vec<_> = (0..n).map(|i| (n - i) as f32).collect();
    let result = gpu_dot(&a, &b)?;
    println!("GPU dot = {}", result);
    Ok(())
}
```

* **Pros:**

  * Massive parallel throughput when `n` is huge.
  * Offloads work from CPU.
* **Cons:**

  * **Data transfer** to/from GPU memory can dominate if `n` is small.
  * Requires OpenCL drivers and setup; more complex to debug.

---

## 4. Embedded (no\_std + Fixed-Point)

On a microcontroller without an FPU or heap, you might:

* Use stack‐allocated arrays (`[i32; N]`)
* Represent “floats” as Q15 fixed-point (`i16` scaled by 2¹⁵)
* Unroll loops and avoid dynamic checks

```rust
#![no_std]
#![no_main]

use cortex_m_rt::entry;

// Q15 fixed-point multiply: (a*b)>>15
fn mul_q15(a: i16, b: i16) -> i16 {
    ((a as i32 * b as i32) >> 15) as i16
}

fn dot_q15(a: &[i16], b: &[i16]) -> i16 {
    let mut sum: i32 = 0;
    for i in 0..a.len() {
        sum += (a[i] as i32 * b[i] as i32);
    }
    // shift back by 15 and saturate
    (sum >> 15).clamp(i16::MIN as i32, i16::MAX as i32) as i16
}

#[entry]
fn main() -> ! {
    // Example: two length-128 vectors in Q15
    let a: [i16; 128] = [ /* ... */ ];
    let b: [i16; 128] = [ /* ... */ ];
    let result = dot_q15(&a, &b);
    // send `result` over UART or toggle an LED based on value
    loop {}
}
```

* **Pros:**

  * Tiny code size, no heap.
  * Works on MCUs without FPUs.
* **Cons:**

  * Lower precision; you must watch overflow.
  * No threading or SIMD.

---

## 5. Trade-Offs & When to Use What

| Target                      | Strengths                                           | Limitations                                    |
| --------------------------- | --------------------------------------------------- | ---------------------------------------------- |
| **CPU (single)**            | Simplicity, predictable latency                     | Doesn’t exploit multiple cores or vector units |
| **CPU (multi-core + SIMD)** | High throughput on medium data; uses existing cores | Task-spawn overhead; alignment concerns        |
| **GPU**                     | Massive data-parallel speed-ups for large N         | Transfer overhead; complex setup/debug         |
| **Embedded**                | Minimal footprint, real-time determinism            | No parallelism, limited precision/memory       |

* **CPU single**: Best for small to medium problems where ease of implementation and low-latency matter.
* **CPU parallel**: Sweet spot when `n` is large enough (millions of elements) and you can pay the threading/SIMD overhead.
* **GPU**: Win when you can batch up huge workloads that dwarf the transfer overhead—e.g. ML training, physics sims.
* **Embedded**: Essential when you’re constrained by memory, power, or lack any OS: go minimal, no heap, fixed-point math.

---

**Bottom line:** matching algorithm design to hardware often gives bigger wins than micro-op tuning. Think first: “How can I expose parallelism? How can I respect memory hierarchies? What resources (FPU, cache, threads) are available?” Then pick the right flavor of implementation.
