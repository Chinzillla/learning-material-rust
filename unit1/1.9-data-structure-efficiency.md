# Data Structure Influence on the Efficiency of Algorithms

Task: Explore how data structures influence the efficiency of algorithms. Provide examples of implementing common data structures in Rust, and discuss how they interact with algorithms like search or sorting operations.

Algorithm efficiency often comes down to choosing the right data structure: the same high-level “what” (e.g. “search for x” or “sort these items”) can cost you anywhere from O(1) to O(n²) depending on how you organize your data. Let’s look at a few common Rust collections (and hand-rolled structures) and see how they pair with typical algorithms.

---

## 1. `Vec<T>` (growable array)

**Strengths:**

* O(1) random access
* Amortized O(1) push/pop at the end
* Excellent locality of reference (cache-friendly)

**Drawbacks:**

* Inserting/removing in the middle is O(n)

```rust
fn vec_example() {
    let mut v = vec![3, 1, 4, 1, 5, 9];
    // sort in-place: average & worst O(n log n)
    v.sort_unstable();
    // binary search: requires sorted data, O(log n)
    match v.binary_search(&4) {
        Ok(idx) => println!("Found 4 at {}", idx),
        Err(_)  => println!("4 not found"),
    }
}
```

* **When to use:** any time you need fast indexing or batch sorting.

---

## 2. Singly Linked List

**Strengths:**

* O(1) insertion/removal at the front
* No reallocations or shifting of elements

**Drawbacks:**

* O(n) lookup by index or value
* Poor cache locality

```rust
// A minimal singly-linked list for demonstration
struct Node<T> {
    value: T,
    next: Option<Box<Node<T>>>,
}

struct LinkedList<T> {
    head: Option<Box<Node<T>>>,
}

impl<T> LinkedList<T> {
    fn new() -> Self { LinkedList { head: None } }

    fn push_front(&mut self, val: T) {
        let new_node = Box::new(Node { value: val, next: self.head.take() });
        self.head = Some(new_node);
    }

    // Linear search: O(n)
    fn contains(&self, target: &T) -> bool
    where
        T: PartialEq,
    {
        let mut cur = &self.head;
        while let Some(node) = cur {
            if &node.value == target {
                return true;
            }
            cur = &node.next;
        }
        false
    }
}

fn list_example() {
    let mut list = LinkedList::new();
    list.push_front(1);
    list.push_front(2);
    println!("Contains 1? {}", list.contains(&1)); // true
    println!("Contains 3? {}", list.contains(&3)); // false
}
```

* **Why it matters:** you couldn’t binary-search or sort this with a slice method—you’d need a merge sort that splits the list, costing O(n log n) time and O(log n) extra stack (or O(1) extra via pointer juggling).

---

## 3. HashMap vs. BTreeMap

| Structure  | Lookup/Insert        | Orderings     |
| ---------- | -------------------- | ------------- |
| `HashMap`  | O(1) avg, O(n) worst | Unordered     |
| `BTreeMap` | O(log n)             | Sorted by key |

```rust
use std::collections::{HashMap, BTreeMap};

fn map_example() {
    let mut hm = HashMap::new();
    hm.insert("apple", 3);
    println!("apple count: {:?}", hm.get("apple"));

    let mut bt = BTreeMap::new();
    bt.insert("banana", 2);
    for (k, v) in bt.iter() {  // in-order traversal
        println!("{} -> {}", k, v);
    }
}
```

* **Search:**

  * `hm.get(&key)` is (amortized) O(1), so it’s great for caches or memo tables.
  * `bt.get(&key)` is O(log n), but you can also do range queries (`range("a".."m")`) cheaply.

---

## 4. `BinaryHeap<T>`

Under the hood, a binary heap is just a `Vec<T>` that maintains the heap property.

* **Push/pop**: O(log n)
* **Peek max/min**: O(1)

```rust
use std::collections::BinaryHeap;

fn heap_example() {
    let mut heap = BinaryHeap::new();
    heap.push(5);
    heap.push(2);
    heap.push(8);

    while let Some(x) = heap.pop() {
        println!("{}", x); // 8, 5, 2
    }
}
```

* **Use case:** priority queues (e.g. Dijkstra’s shortest paths), event simulation, partial sorting (take the k largest).

---

## 5. How Structure Guides Algorithm Design

| Operation      | Best DS                    | Complexity                   |
| -------------- | -------------------------- | ---------------------------- |
| Random access  | `Vec<T>`                   | O(1)                         |
| Prefix scan    | `Vec<T>`, `VecDeque<T>`    | O(1) per element             |
| FIFO queue     | `VecDeque<T>`              | O(1) push/pop both ends      |
| LIFO stack     | `Vec<T>`                   | O(1) push/pop end            |
| Sorted insert  | `BTreeMap` or `BinaryHeap` | O(log n)                     |
| Exact lookup   | `HashMap<T, U>`            | O(1) avg                     |
| Range queries  | `BTreeMap`                 | O(log n + k)                 |
| Top-k elements | `BinaryHeap`               | O(n + k log n) or O(n log k) |

* **Binary search** only works on random-access slices (`Vec` or arrays), never on linked lists.
* **Sorting** a `Vec` is built-in; sorting a list requires a pointer-based mergesort.
* **Graph algorithms** pick adjacency lists (`Vec<Vec<_>>`) for speed or adjacency maps (`HashMap<_, Vec<_>>`) for sparse or dynamic graphs.

---

### Bottom Line

The choice of data structure isn’t just academic—it directly controls which algorithms you can apply and how fast they’ll run:

* Need O(1) lookups? reach for a `HashMap`.
* Need ordered iteration? `BTreeMap` or a sorted `Vec`.
* Need constant-time queue operations? `VecDeque`.
* Need priority scheduling? `BinaryHeap`.

Pair your problem’s “common operations” with the structure that makes those operations cheap, and you’ll turn an O(n²) disaster into O(n log n) or better.
