# Big O Notation

Task: Analyze how algorithms have evolved in terms of complexity and efficiency over time. Provide examples of simple and complex algorithms implemented in Rust, and use Big O notation to compare their time and space complexities.

## 1. A Very Simple Algorithm: Linear Search

Scan each element until you hit your target.

```rust
/// O(n) time, O(1) space
fn linear_search<T: PartialEq>(arr: &[T], target: &T) -> Option<usize> {
    for (i, item) in arr.iter().enumerate() {
        if item == target {
            return Some(i);
        }
    }
    None
}

fn main() {
    let data = [3, 7, 1, 9, 2];
    println!("{:?}", linear_search(&data, &9));  // Some(3)
}
```

* **Time:** O(n)
* **Space:** O(1)
* **When it was common:** In the early days—when data sets were tiny—brute-force scans were fine.

---

## 2. A More “Complex” Divide-and-Conquer: Merge Sort

Split in half, sort each recursively, then merge.

```rust
/// O(n log n) time, O(n) space
fn merge_sort<T: Ord + Clone>(arr: &[T]) -> Vec<T> {
    let len = arr.len();
    if len <= 1 { return arr.to_vec() }

    let mid = len / 2;
    let left = merge_sort(&arr[..mid]);
    let right = merge_sort(&arr[mid..]);

    // merge
    let mut merged = Vec::with_capacity(len);
    let (mut i, mut j) = (0, 0);
    while i < left.len() && j < right.len() {
        if left[i] <= right[j] {
            merged.push(left[i].clone());
            i += 1;
        } else {
            merged.push(right[j].clone());
            j += 1;
        }
    }
    merged.extend_from_slice(&left[i..]);
    merged.extend_from_slice(&right[j..]);
    merged
}

fn main() {
    let data = vec![5, 2, 9, 1, 5, 6];
    let sorted = merge_sort(&data);
    println!("{:?}", sorted);  // [1, 2, 5, 5, 6, 9]
}
```

* **Time:** O(n log n) in all cases
* **Space:** O(n) (for temporary buffers)
* **Why it mattered:** Once datasets grew, n² sorts (bubble, selection) became too slow—merge sort’s divide-and-conquer was a game changer.

---

## 3. Peeking at the Evolution

| Era                                   | Typical Algorithms         | Time / Space          |
| ------------------------------------- | -------------------------- | --------------------- |
| **1950s–60s: Brute force**            | Linear search, bubble sort | O(n), O(n²) / O(1)    |
| **1960s–70s: Divide & conquer**       | Merge sort, quicksort      | O(n log n) / O(n)     |
| **1970s–80s: Dynamic programming**    | Knapsack, matrix chain     | polynomial / O(n²)    |
| **1980s–90s: Graph & number theory**  | Dijkstra’s, elliptic-curve | O(E + V log V), O(n³) |
| **2000s–today: Sublinear & parallel** | FFT, streaming sketches    | O(n log n), O(log n)  |

1. **Brute force → binary / divide-&-conquer.**

   * Early code literally tried every possibility.
   * Then came binary search (O(log n)), merge/quicksort (O(n log n)).

2. **DP & greedy algorithms.**

   * Clever reuse of subproblem results turned exponential recursions into polynomial-time.
   * Examples: edit distance, knapsack.

3. **Specialized structures.**

   * Trees, heaps, hash tables—constant-time ops for insert/remove/look-up.
   * Graph algorithms leveraging priority queues (e.g. Dijkstra’s O(E + V log V)).

4. **Modern frontiers.**

   * Sublinear (sketching, streaming) for massive data.
   * Parallel/distributed (MapReduce, GPUs).
   * Approximation and randomized algorithms when exact is too slow.

---

## 4. What Big-O Teaches Us

* **Time vs. space trade-offs**: Merge sort uses extra memory to get guaranteed n log n time; in-place algorithms (in-place quicksort) save space but risk worst-case slowdowns.
* **Algorithm choice matters**: A n log n algorithm on millions of items is orders of magnitude faster than n².
* **Evolution driven by scale**: As problem sizes ballooned—from kilobytes to petabytes—algorithmic efficiency went from “nice to have” to “mission-critical.”

By looking at just two small Rust functions, you can see how we’ve moved from scanning linearly through data to slicing it in half and conquering each piece. And that shift—born in the 1960s—echoes through every scalable system you use today.
